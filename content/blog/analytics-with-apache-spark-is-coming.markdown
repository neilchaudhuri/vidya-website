---

author: "Neil Chaudhuri"
title: "Analytics With Apache Spark Is Coming"
description: "Learn the hottest technology in data analytics with our Apache Spark course coming in Summer 2015."
banner: "img/banners/spark.png"
date: 2015-04-22
tags:
- Hadoop
- MapReduce
- Scala
- Java
- Python
- Apache Spark
- HBase
categories: 
- Big Data
- Analytics
- Programming
- Functional Programming
- Architecture
- Open-source
aliases:
- /blog/Data/Hadoop/Analytics/Programming/Scala/Java/Python/Architecture/2015/04/22/analytics-with-apache-spark-is-coming
---

At Vidya we currently offer two courses, [Software Engineering in Java](/course/Java/Programming/2015/01/07/software-engineering-in-java)
and [Agile Software Project Management with Scrum](/course/Agile/Scrum/Projects/2015/01/07/agile-software-project-management-with-scrum).
In response to popular demand...OK, like eight or nine people...we are currently working on a third course to be ready
by Summer 2015 tentatively called *Analytics with Apache Spark*.



As “[Big Data](/categories/big-data)” becomes more and more of a thing, there just aren’t enough software engineers who know the tools and
techniques for doing meaningful, performant, cloud-scale analytics. Meanwhile, [Apache Spark](/tags/apache-spark) is
surging in popularity for two reasons. Spark provides a much easier programming model than old-school
[MapReduce](/tags/mapreduce) in [Hadoop](/tags/hadoop), which
is great for developers. And Spark works a lot faster because it optimizes operations over cluster memory rather than
lethargic disc I/O like [MapReduce](tags/mapreduce), which is great for everybody.

Like [Software Engineering in Java](/course/software-engineering-in-java), *Analytics with
Apache Spark* will be heavy on code--both with examples and hands-on exercises. We will use Spark’s [Scala](/tags/scala)
API since the [Java](/tags/java) and [Python](/tags/python) APIs aren’t as complete and performant. If you
don’t know Scala, don’t worry. We will spend time on learning just enough Scala to be dangerous with Spark.

*Analytics with Apache Spark* will feature two three-hour sessions.

The first session focuses on understanding the advantages of Spark for analytics, learning a little Scala, and
mastering the Spark API--particularly the [SparkContext](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)
and the [RDD](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD), Spark’s fundamental
abstraction. We start simple with hardcoded datasets and the [Spark shell](https://spark.apache.org/docs/latest/quick-start.html) and progress to real world datasets and
full-fledged Spark programs. The second session focuses on using Spark in the real world--running and monitoring on a
cluster, integrating with Hadoop (for example [how to use Spark with HBase](/blog/Programming/Scala/Java/Data/Hadoop/Analytics/2014/01/25/lighting-a-spark-with-hbase)),
performance considerations with RDDs, and tuning, testing, and debugging.

The entire course will be taught in a classroom, but eventually there will be an online version as well. 

Some of the details may change, but this is what we currently envision for *Analytics with Apache Spark*. It is hard to
teach such a complex topic in such a short time, but we hope to create a course that gives you the confidence
to market yourself as a Spark expert and to produce great analytics to prove it--and maybe to make some
[cash money](https://www.youtube.com/watch?v=JDwHor1h7L4) doing what you love.

[Let us know](/contact) what you want to see in *Analytics with Apache Spark*. We want to build something you will learn from and enjoy.

